

services:
  naive-ollma:
    build: .
    container_name: naive-ollma-gpu-consultant
    runtime: nvidia
    ports:
      - "11434:11434"  # Ollama API port
    volumes:
      - ./credential.txt:/app/credential.txt:ro  # Mount credentials
      - ./results:/app/results  # Mount results directory
      - ./logs:/app/logs  # Mount logs directory
      - .:/app  # Mount current directory for hopeful_alphas.json sharing
      - ollama_data:/root/.ollama  # Persist Ollama models
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      # ========================================
      # RTX 3060 Ti (8GB VRAM) Settings (User Configured)
      # ========================================
      - OLLAMA_GPU_LAYERS=20              # User configured: 16 layers on GPU
      - OLLAMA_NUM_PARALLEL=1             # Keep at 1 to avoid VRAM fragmentation
      - OLLAMA_GPU_MEMORY_UTILIZATION=0.8 # User configured: 75% VRAM (~6GB usage)
      - OLLAMA_GPU_MEMORY_FRACTION=0.8   # User configured: 75% VRAM
      - OLLAMA_MAX_LOADED_MODELS=1        # Only keep 1 model in VRAM at a time
      - OLLAMA_NUM_GPU=1                  # Use single GPU
      # Enable RAG system and improvements
      - USE_RAG=true                      # Enable RAG system for better alpha quality
      - USE_SMART_CONFIG=true             # Enable smart config selector
      - USE_FEEDBACK_LOOP=true            # Enable feedback loop learning
      # Qdrant Vector Database Configuration
      - USE_QDRANT=true                   # Use Qdrant instead of TF-IDF for RAG
      - QDRANT_HOST=qdrant                # Qdrant service hostname
      - QDRANT_PORT=6333                  # Qdrant REST API port
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          memory: 10G    # User configured: 8G RAM
          cpus: '4.0'   # User configured: 3.0 CPU cores
    restart: unless-stopped
    command: ["python", "alpha_orchestrator.py", "--mode", "continuous", "--batch-size", "1", "--max-concurrent", "1"]
    depends_on:
      - qdrant
    networks:
      - naive-ollma-network

  # Optional: Add a simple web interface for monitoring
  ollama-webui:
    image: ghcr.io/ollama-webui/ollama-webui:main
    container_name: ollama-webui-gpu-consultant
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_API_BASE_URL=http://naive-ollma-gpu:11434/api
    depends_on:
      - naive-ollma
    restart: unless-stopped
    networks:
      - naive-ollma-network
      
  # Machine Miner Service
  machine-miner:
    build: .
    container_name: machine-miner-gpu-consultant
    runtime: nvidia
    volumes:
      - ./credential.txt:/app/credential.txt:ro
      - ./results:/app/results
      - ./logs:/app/logs
      - .:/app
    environment:
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          memory: 6G
          cpus: '1.5'
    restart: unless-stopped
    command: ["python", "machine_miner.py", "--credentials", "/app/credential.txt", "--region", "USA", "--universe", "TOP3000"]
    depends_on:
      - naive-ollma
    networks:
      - naive-ollma-network

  # Alpha Generator Dashboard
  alpha-dashboard:
    build: .
    container_name: alpha-dashboard-gpu-consultant
    ports:
      - "5000:5000"
    volumes:
      - ./results:/app/results
      - ./logs:/app/logs
      - ./credential.txt:/app/credential.txt:ro
      - .:/app  # Mount current directory for hopeful_alphas.json sharing
    environment:
      - FLASK_ENV=production
      - PYTHONUNBUFFERED=1
    command: ["python", "web_dashboard.py"]
    depends_on:
      - naive-ollma
    restart: unless-stopped
    networks:
      - naive-ollma-network

  # Qdrant Vector Database Service
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant-vector-db
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      - qdrant_storage:/qdrant/storage  # Persist vector data
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__SERVICE__HTTP_PORT=6333
    restart: unless-stopped
    networks:
      - naive-ollma-network

volumes:
  ollama_data:
    driver: local
  qdrant_storage:
    driver: local

networks:
  naive-ollma-network:
    driver: bridge
